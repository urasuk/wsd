import json
from collections import defaultdict
import stats as stats_data
import ast
import re

input_file_path = stats_data.lcopf_forms_found_file_path

# bad
# pairs_file_path = stats_data.pairs_file_path

# good
train_forms_file_path = stats_data.train_forms_file_path
test_forms_file_path = stats_data.test_forms_file_path


train_good_file_path = stats_data.train_lem_file_path
train_bad_file_path = stats_data.train_not_lem_file_path 

test_good_file_path = stats_data.test_lem_file_path
test_bad_file_path = stats_data.test_not_lem_file_path 


class Lemmatizer:
    def __init__(self, jsonl_file_path):
        self.lemmatizer_map = {}
        self.load_from_jsonl(jsonl_file_path)

    def load_from_jsonl(self, jsonl_file_path):
        with open(jsonl_file_path, 'r', encoding='utf-8') as file:
            for line in file:
                entry = json.loads(line)
                lemma = entry.get('lemma')
                forms = entry.get('forms')
                if lemma and forms:
                    if type(forms) != list:
                        forms = ast.literal_eval(forms)
                    self.lemmatizer_map[lemma] = forms

    def is_target_in_lemma_forms(self, target, lemma):
        try:
            forms = self.lemmatizer_map.get(lemma, [])
            return target in forms 
        except Exception as e:
            print(f"ü•∂ –ü–†–û–ë–õ–ï–ú–ê –©–û –õ–ï–ú–ò –ú–ê–Æ–¢–¨ –í synonyms —Ç–∞–∫—ñ –ª–µ–º–∏, —è–∫–∏—Ö –Ω–µ–º–∞ –≤ merged_df: {e}") # –Ω–∞–ø—Ä–∏–∫–ª–∞–¥ –ó–ê–ú–ÜÃÅ–†–ò–¢–ò
            return False
    
    def remove_punctuation(self, text, lemma):
        # Define the punctuation characters to remove
        punctuation_to_remove = set(";:,.!\"?/‚Äú‚Äù()¬´¬ª") # -

        # Add "-" to punctuation_to_remove if lemma contains "-"
        # if '-' in lemma:
        #     punctuation_to_remove.remove('-')

        result = []

        for char in text:
            if char not in punctuation_to_remove: # or (char == '-' and '-' in result)
                result.append(char)

        return ''.join(result)

    def get_target_idx(self, sentence_input, lemma):
        sentence = sentence_input.lower()
        # Tokenize the sentence into words
        words = [self.remove_punctuation(word, lemma) for word in sentence.split()]

        target_start_idx = -1
        target_end_idx = -1
        # target_start_pos = -1
        # target_end_pos = -1

        if lemma == '–∑–∞–º—ñ—Ä–∏—Ç–∏':
            print("bug")

        # –∞—à–≥–∞–±–∞—Ç—Å—å–∫–∏–π –∞–±–∞—Ç—Å—å–∫–∏–π ü§í
        counter = 0
        for i, word in enumerate(words):
            # if word.startswith(lemma):
            if self.is_target_in_lemma_forms(word, lemma) or lemma in word: 
                #? or lemma in word:  –∞–≤—Ç–æ–∫—Ä–∞—Ç: –ø—Ä–∞–≤–∏—Ç–µ–ª—è-–∞–≤—Ç–æ–∫—Ä–∞—Ç–∞ –ø—Ä–∞–≤–∏—Ç–µ–ª—è–∞–≤—Ç–æ–∫—Ä–∞—Ç–∞
                # target_start_idx = i
                # target_end_idx = i + len(word.split("-"))  # Consider hyphenated words
                target_start_idx = sentence.find(word) # , target_start_pos + 1
                target_end_idx = target_start_idx + len(word) - 1

                counter+=1
                if counter > 2:
                    print(f'ü§¢ {sentence} | {lemma}')
                # ! –≤–∏–ø–∞–¥–æ–∫ –∫–æ–ª–∏ –≤ —Ä–µ—á–µ–Ω—ñ –¥–µ–∫—ñ–ª—å–∫–∞ –ª–µ–º , —Ç—Ä–µ–±–∞ –±—Ä–∞—Ç–∏ –º–∞–±—É—Ç—å —É—Å—ñ—Ö —ó—Ö –ø–æ–∑–∏—Ü—ñ—ó —â–æ–± –º–æ–¥–µ–ª—å –≤—Å—ñ –≤—Ä–∞—Ö–æ–≤—É–≤–∞–ª–∞
                # ü§¢ –¥–∑–≤–æ–Ω–æ–º —Å–∫–ª–∏–∫–∞–≤—Å—è –Ω–∞—Ä–æ–¥ –Ω–∞ –≤—ñ—á–µ. –¥–∑–≤–æ–Ω–æ–º –≤–∫–∞–∑—É–≤–∞–ª–∏ –¥–æ—Ä–æ–≥—É –ø–æ–¥–æ—Ä–æ–∂–Ω—ñ–º, —â–æ –∑–∞–±–ª—É–∫–∞–ª–∏ –≤ –Ω–µ–≥–æ–¥—ñ. –¥–∑–≤–æ–Ω–æ–º –æ–ø–æ–≤—ñ—â–∞–≤—Å—è –Ω–∞—Ä–æ–¥ –ø—Ä–æ –ø–µ—Ä–µ–º–æ–≥—É —ñ –≤—ñ—Ç–∞–ª–æ—Å—è –ø–µ—Ä–µ–º–æ–∂–Ω–µ –ø–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è –ø–æ–ª–∫—ñ–≤ –∑ –≤—ñ–π–Ω–∏ | –¥–∑–≤—ñ–Ω
                
        if counter == 0: return False

        return {
            "target_start_idx": target_start_idx,
            "target_end_idx": target_end_idx,
            # "target_start_pos": target_start_pos,
            # "target_end_pos": target_end_pos
        }


def df_lemmantization(pairs_df, lemmatizer):
    good_data = []
    bad_data = []

    for i, pair in enumerate(pairs_df):
        sent1 = pair["sentence1"]
        sent2 = pair["sentence2"]
        lemma = pair["lemma"]

        # if lemma == '–∞–≤—Ç–æ–∫—Ä–∞—Ç' and \
        #    sent1 == '–ü–µ—Ä–µ–¥ —Å–≤–æ—ó–º –≤—Ç–æ—Ä–≥–Ω–µ–Ω–Ω—è–º –¥–æ –ì—Ä–µ—Ü—ñ—ó —Å—Ö—ñ–¥–Ω–∏–π –∞–≤—Ç–æ–∫—Ä–∞—Ç –Ω–∞—Å–ª—É—Ö–∞–≤—Å—è –ø–æ—Ä–∞–¥, —è–∫ –ø–æ–∑–±–∞–≤–∏—Ç–∏ –µ–ª–ª—ñ–Ω—ñ–≤ –ø—Ä–æ–¥–æ–≤–æ–ª—å—á–æ—ó –±–∞–∑–∏, —ñ —Ç–≤–µ—Ä–µ–∑–æ –ø–µ—Ä–µ—Å–ª—ñ–¥—É–≤–∞–≤ —Ü—é —Å—Ç—Ä–∞—Ç–µ–≥—ñ—á–Ω—É –º–µ—Ç—É' and \
        #    sent2 == '–ë—ñ–ª—å—à–∞ —á–∞—Å—Ç–∏–Ω–∞ –¥–µ—Ä–∂–∞–≤–Ω–∏—Ö —Ä–µ—Å—É—Ä—Å—ñ–≤ —Å—Ç–∞–ª–∞ –æ—Å–æ–±–∏—Å—Ç–æ—é –≤–ª–∞—Å–Ω—ñ—Å—Ç—é –ø—Ä–∞–≤–∏—Ç–µ–ª—è-–∞–≤—Ç–æ–∫—Ä–∞—Ç–∞ —ñ –π–æ–≥–æ –Ω–∞–π–±–ª–∏–∂—á–æ–≥–æ –æ—Ç–æ—á–µ–Ω–Ω—è':
        #     print("#")
        synonyms = pair["synonyms"]
        sent1_target_pos = False
        sent2_target_pos = False
        for synonym in synonyms:
            if not sent1_target_pos:
                sent1_target_pos = lemmatizer.get_target_idx(sent1, synonym["lemma"])
            if not sent2_target_pos:
                sent2_target_pos = lemmatizer.get_target_idx(sent2, synonym["lemma"])

        if sent1_target_pos == False:
            # print(f'NO POSITIONS FOUND FOR SENT_1 | {sent1} | {lemma}')
            pass
            
        if sent2_target_pos == False:
            # print(f'NO POSITIONS FOUND FOR SENT_2 | {sent2} | {lemma}')
            pass
            
        if sent1_target_pos == False or sent2_target_pos == False:
            bad_data.append(pair)
            continue
        
        # If both positions are found, add to good_data
        pair["sent1_target_pos"] = sent1_target_pos
        pair["sent2_target_pos"] = sent2_target_pos
        good_data.append(pair)

    return good_data, bad_data
        


def main():
    lemmatizer = Lemmatizer(input_file_path)

    # Lemmatize a word
    # res = lemmatizer.get_target_idx(".,–ª—ñ–∫–∞—Ä–∫–∞-—Ç–æ–º—É? –ø–æ—Ç—è–≥–∞–º–∏,.", "–ø–æ—Ç—è–≥")
    # print(res)

    # =============================================================================

    with open(test_forms_file_path, 'r', encoding='utf-8') as file:
        pairs_test_df = [json.loads(line) for line in file]

    test_good, test_bad = df_lemmantization(pairs_test_df, lemmatizer)

    # Write the test set to a new file
    with open(test_good_file_path, 'w', encoding='utf-8') as output_file:
        for entry in test_good:
            json.dump(entry, output_file, ensure_ascii=False)
            output_file.write('\n')

    # Write the test set to a new file
    with open(test_bad_file_path, 'w', encoding='utf-8') as output_file:
        for entry in test_bad:
            json.dump(entry, output_file, ensure_ascii=False)
            output_file.write('\n')

    #* Test: good [267_913] | bad [[161_25]]
    print(f"Test: good [{len(test_good)}] | bad [[{len(test_bad)}]]")
    # =============================================================================

    with open(train_forms_file_path, 'r', encoding='utf-8') as file:
        pairs_train_df = [json.loads(line) for line in file]

    train_good, train_bad = df_lemmantization(pairs_train_df, lemmatizer)

    # Write the train set to a new file
    with open(train_good_file_path, 'w', encoding='utf-8') as output_file:
        for entry in train_good:
            json.dump(entry, output_file, ensure_ascii=False)
            output_file.write('\n')

    # Write the train set to a new file
    with open(train_bad_file_path, 'w', encoding='utf-8') as output_file:
        for entry in train_bad:
            json.dump(entry, output_file, ensure_ascii=False)
            output_file.write('\n')

    #* Train: good [802_065] | bad [[50_045]]
    print(f"Train: good [{len(train_good)}] | bad [[{len(train_bad)}]]")
    # =============================================================================



    # with open(pairs_file_path, 'r', encoding='utf-8') as file:
    #     pairs_data = [json.loads(line) for line in file]

    # for line in pairs_data:
    #     lemma, pairs = next(iter(line.items()))
    #     # line_expanded = {
    #     #     "lemma": lemma,
    #     #     "pairs": pairs,
    #     #     "positive_pairs": [],
    #     #     "negative_pairs": [],
    #     #     }
 
    #     for i, pair in enumerate(pairs):
    #         sent1 = pair["sentence1"]
    #         sent2 = pair["sentence2"]
    #         lemma = pair["lemma"]

    #         # if lemma == '–∞–≤—Ç–æ–∫—Ä–∞—Ç' and \
    #         #    sent1 == '–ü–µ—Ä–µ–¥ —Å–≤–æ—ó–º –≤—Ç–æ—Ä–≥–Ω–µ–Ω–Ω—è–º –¥–æ –ì—Ä–µ—Ü—ñ—ó —Å—Ö—ñ–¥–Ω–∏–π –∞–≤—Ç–æ–∫—Ä–∞—Ç –Ω–∞—Å–ª—É—Ö–∞–≤—Å—è –ø–æ—Ä–∞–¥, —è–∫ –ø–æ–∑–±–∞–≤–∏—Ç–∏ –µ–ª–ª—ñ–Ω—ñ–≤ –ø—Ä–æ–¥–æ–≤–æ–ª—å—á–æ—ó –±–∞–∑–∏, —ñ —Ç–≤–µ—Ä–µ–∑–æ –ø–µ—Ä–µ—Å–ª—ñ–¥—É–≤–∞–≤ —Ü—é —Å—Ç—Ä–∞—Ç–µ–≥—ñ—á–Ω—É –º–µ—Ç—É' and \
    #         #    sent2 == '–ë—ñ–ª—å—à–∞ —á–∞—Å—Ç–∏–Ω–∞ –¥–µ—Ä–∂–∞–≤–Ω–∏—Ö —Ä–µ—Å—É—Ä—Å—ñ–≤ —Å—Ç–∞–ª–∞ –æ—Å–æ–±–∏—Å—Ç–æ—é –≤–ª–∞—Å–Ω—ñ—Å—Ç—é –ø—Ä–∞–≤–∏—Ç–µ–ª—è-–∞–≤—Ç–æ–∫—Ä–∞—Ç–∞ —ñ –π–æ–≥–æ –Ω–∞–π–±–ª–∏–∂—á–æ–≥–æ –æ—Ç–æ—á–µ–Ω–Ω—è':
    #         #     print("#")
    #         synonyms = pair["synonyms"]
    #         sent1_target_pos = False
    #         sent2_target_pos = False
    #         for synonym in synonyms:
    #             if not sent1_target_pos:
    #                 sent1_target_pos = lemmatizer.get_target_idx(sent1, synonym["lemma"])
    #             if not sent2_target_pos:
    #                 sent2_target_pos = lemmatizer.get_target_idx(sent2, synonym["lemma"])

    #         if sent1_target_pos == False:
    #             print(f'NO POSITIONS FOUND FOR SENT_1 | {sent1} | {lemma}')
    #         if sent2_target_pos == False:
    #             print(f'NO POSITIONS FOUND FOR SENT_2 | {sent2} | {lemma}')
            

if __name__ == '__main__':
    main()