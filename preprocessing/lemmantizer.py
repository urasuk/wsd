import json
from collections import defaultdict
import stats as stats_data
import ast
import re

input_file_path = stats_data.lcopf_forms_found_file_path

# bad
# pairs_file_path = stats_data.pairs_file_path

# good
# train_forms_file_path = stats_data.train_forms_file_path
# test_forms_file_path = stats_data.test_forms_file_path
train_forms_file_path = stats_data.train_forms_deduplicated_file_path
test_forms_file_path = stats_data.test_forms_deduplicated_file_path


# train_good_file_path = stats_data.train_lem_file_path
# train_bad_file_path = stats_data.train_not_lem_file_path 

# test_good_file_path = stats_data.test_lem_file_path
# test_bad_file_path = stats_data.test_not_lem_file_path 

train_good_file_path = stats_data.train_lem__dedup_file_path
train_bad_file_path = stats_data.train_not_lem__dedup_file_path 

test_good_file_path = stats_data.test_lem__dedup_file_path
test_bad_file_path = stats_data.test_not_lem__dedup_file_path 


class Lemmatizer:
    def __init__(self, jsonl_file_path):
        self.lemmatizer_map = {}
        self.load_from_jsonl(jsonl_file_path)

    def load_from_jsonl(self, jsonl_file_path):
        with open(jsonl_file_path, 'r', encoding='utf-8') as file:
            for line in file:
                entry = json.loads(line)
                lemma = entry.get('lemma')
                forms = entry.get('forms')
                if lemma and forms:
                    if type(forms) != list:
                        forms = ast.literal_eval(forms)
                    self.lemmatizer_map[lemma] = forms

    def is_target_in_lemma_forms(self, target, lemma):
        try:
            forms = self.lemmatizer_map.get(lemma, [])
            return target in forms 
        except Exception as e:
            print(f"ü•∂ –ü–†–û–ë–õ–ï–ú–ê –©–û –õ–ï–ú–ò –ú–ê–Æ–¢–¨ –í synonyms —Ç–∞–∫—ñ –ª–µ–º–∏, —è–∫–∏—Ö –Ω–µ–º–∞ –≤ merged_df: {e}") # –Ω–∞–ø—Ä–∏–∫–ª–∞–¥ –ó–ê–ú–ÜÃÅ–†–ò–¢–ò
            return False
    
    def remove_punctuation(self, text):
        # Define the punctuation characters to remove
        punctuation_to_remove = set(";:,.!\"?/‚Äú‚Äù()¬´¬ª") # -

        # Add "-" to punctuation_to_remove if lemma contains "-"
        # if '-' in lemma:
        #     punctuation_to_remove.remove('-')

        result = []

        for char in text:
            if char not in punctuation_to_remove: # or (char == '-' and '-' in result)
                result.append(char)

        return ''.join(result)

    def remove_extra_spaces(self, sentence):
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ä–µ–≥—É–ª—è—Ä–Ω–∏–π –≤–∏—Ä–∞–∑ –¥–ª—è –∑–∞–º—ñ–Ω–∏ –¥–≤–æ—Ö –∞–±–æ –±—ñ–ª—å—à–µ –ø—Ä–æ–±—ñ–ª—ñ–≤ –ø—ñ–¥—Ä—è–¥ –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–±—ñ–ª
        return re.sub(r'\s{2,}', ' ', sentence).strip()

    def find_targets_general_position(self, sentence):
        words = sentence.split()
        indices = []
        start = 0
        for w in words:
            end = start + len(w) - 1
            if len(w) >= 3:  # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∑–∞ –¥–æ–≤–∂–∏–Ω–æ—é —Å–ª–æ–≤–∞
                indices.append((w, start, end))
            start = end + 2  # –¥–æ–¥–∞—î–º–æ 1 –∑–∞ —Å–ª–æ–≤–æ–º —ñ 1 –∑–∞ –ø—Ä–æ–±—ñ–ª–æ–º
        return indices

    def find_targets_specific_position(self, targets_with_indeces):
        result_data = []
        for target, idx_start_gen, idx_end_gen in targets_with_indeces:
            target_cleaned = self.remove_punctuation(target)
            idx_start_sub = target.find(target_cleaned)
            idx_end_sub = idx_start_sub + len(target_cleaned) - 1
            result_data.append((target_cleaned,
                               idx_start_gen+idx_start_sub,
                               idx_start_gen+idx_end_sub))
        return result_data


    def get_target_idx(self, sentence_input, lemma):
        # sentence = sentence_input.lower() #? üìç
        sentence = sentence_input

        targets_with_idxs_gen = self.find_targets_general_position(sentence)
        targets_with_idxs = self.find_targets_specific_position(targets_with_idxs_gen)

        # Tokenize the sentence into words
        # targets = [self.remove_punctuation(word) for word in sentence.split()]

        target_start_idx = -1
        target_end_idx = -1

        # if lemma == '–∑–∞–º—ñ—Ä–∏—Ç–∏':
        #     print("bug")

        # –∞—à–≥–∞–±–∞—Ç—Å—å–∫–∏–π –∞–±–∞—Ç—Å—å–∫–∏–π ü§í
        counter = 0
        for i, (target, s_idx, e_idx) in enumerate(targets_with_idxs):
            target_lower = target.lower() #? üìç
            if self.is_target_in_lemma_forms(target_lower, lemma) \
               or (lemma in target_lower and target_lower.find('-') != -1 and lemma.find('-') != -1): 
               #? –ø—Ä–æ–±—É—é –Ω–æ–≤–∏–π –ø—ñ–¥—Ö—ñ–¥
                
                # or lemma in word:
                #? or lemma in word:  –∞–≤—Ç–æ–∫—Ä–∞—Ç: –ø—Ä–∞–≤–∏—Ç–µ–ª—è-–∞–≤—Ç–æ–∫—Ä–∞—Ç–∞ –ø—Ä–∞–≤–∏—Ç–µ–ª—è–∞–≤—Ç–æ–∫—Ä–∞—Ç–∞
                #! ‚≠êÔ∏è –ø—Ä–∏–∑–≤–µ–ª–æ –¥–æ –Ω–∞—Å—Ç—É–ø–Ω–æ—ó –ø—Ä–æ–±–ª–µ–º–∏:
                # sent1 –ü—Ä–æ–≤–µ—Å—Ç–∏ —Å—É–¥–Ω–æ —á–µ—Ä–µ–∑ —Å–∫–µ–ª—è—Å—Ç—ñ –≤—É–∑—å–∫—ñ ‚úÖ–≤–æ—Ä–æ—Ç–∞‚úÖ –∑ –ø—ñ–¥–≤–æ–¥–Ω–∏–º–∏ —Ä–∏—Ñ–∞–º–∏ —ñ –º—ñ–ª–∏–Ω–∞–º–∏ –≤–∏–º–∞–≥–∞–ª–æ –≤—ñ–¥ –∫–∞–ø—ñ—Ç–∞–Ω–∞ –≤–µ–ª–∏–∫–æ—ó –º–∞–π—Å—Ç–µ—Ä–Ω–æ—Å—Ç—ñ —Ç–∞ –¥–æ—Å–≤—ñ–¥—É
                # sent2 –•–ª–æ–ø—Ü—ñ –≥—Ä–∞–ª–∏ —É —Ñ—É—Ç–±–æ–ª–∞  . ‚õîÔ∏è–í–æ—Ä–æ—Ç–∞‚õîÔ∏è—Ä–µ–º –±—É–≤ –í–∞—Å–∏–ª—å... –ú'—è—á –ª–µ—Ç—ñ–≤ –ø—Ä–æ—Å—Ç–æ –Ω–∞ ‚úÖ–≤–æ—Ä–æ—Ç–∞‚úÖ


                # target_start_idx = i
                # target_end_idx = i + len(target.split("-"))  # Consider hyphenated words

                #! ‚≠êÔ∏è another problem
                # target_start_idx = sentence.find(target) # , target_start_pos + 1
                # target_end_idx = target_start_idx + len(target) - 1
                target_start_idx = s_idx
                target_end_idx = e_idx

                counter+=1
                if counter > 2:
                    print(f'ü§¢ {sentence} | {lemma}')
                    break
                # ! –≤–∏–ø–∞–¥–æ–∫ –∫–æ–ª–∏ –≤ —Ä–µ—á–µ–Ω—ñ –¥–µ–∫—ñ–ª—å–∫–∞ –ª–µ–º , —Ç—Ä–µ–±–∞ –±—Ä–∞—Ç–∏ –º–∞–±—É—Ç—å —É—Å—ñ—Ö —ó—Ö –ø–æ–∑–∏—Ü—ñ—ó —â–æ–± –º–æ–¥–µ–ª—å –≤—Å—ñ –≤—Ä–∞—Ö–æ–≤—É–≤–∞–ª–∞
                # ü§¢ –¥–∑–≤–æ–Ω–æ–º —Å–∫–ª–∏–∫–∞–≤—Å—è –Ω–∞—Ä–æ–¥ –Ω–∞ –≤—ñ—á–µ. –¥–∑–≤–æ–Ω–æ–º –≤–∫–∞–∑—É–≤–∞–ª–∏ –¥–æ—Ä–æ–≥—É –ø–æ–¥–æ—Ä–æ–∂–Ω—ñ–º, —â–æ –∑–∞–±–ª—É–∫–∞–ª–∏ –≤ –Ω–µ–≥–æ–¥—ñ. –¥–∑–≤–æ–Ω–æ–º –æ–ø–æ–≤—ñ—â–∞–≤—Å—è –Ω–∞—Ä–æ–¥ –ø—Ä–æ –ø–µ—Ä–µ–º–æ–≥—É —ñ –≤—ñ—Ç–∞–ª–æ—Å—è –ø–µ—Ä–µ–º–æ–∂–Ω–µ –ø–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è –ø–æ–ª–∫—ñ–≤ –∑ –≤—ñ–π–Ω–∏ | –¥–∑–≤—ñ–Ω
                
        if counter == 0: return False

        return {
            "target_start_idx": target_start_idx,
            "target_end_idx": target_end_idx,
        }


def df_lemmantization(pairs_df, lemmatizer):
    good_data = []
    bad_data = []

    for i, pair in enumerate(pairs_df):
        pair["sentence1"] = lemmatizer.remove_extra_spaces(pair["sentence1"])
        pair["sentence2"] = lemmatizer.remove_extra_spaces(pair["sentence2"])
        sent1 = pair["sentence1"]
        sent2 = pair["sentence2"]
        # lemma = pair["lemma"]

        # if lemma == '–∞–≤—Ç–æ–∫—Ä–∞—Ç' and \
        #    sent1 == '–ü–µ—Ä–µ–¥ —Å–≤–æ—ó–º –≤—Ç–æ—Ä–≥–Ω–µ–Ω–Ω—è–º –¥–æ –ì—Ä–µ—Ü—ñ—ó —Å—Ö—ñ–¥–Ω–∏–π –∞–≤—Ç–æ–∫—Ä–∞—Ç –Ω–∞—Å–ª—É—Ö–∞–≤—Å—è –ø–æ—Ä–∞–¥, —è–∫ –ø–æ–∑–±–∞–≤–∏—Ç–∏ –µ–ª–ª—ñ–Ω—ñ–≤ –ø—Ä–æ–¥–æ–≤–æ–ª—å—á–æ—ó –±–∞–∑–∏, —ñ —Ç–≤–µ—Ä–µ–∑–æ –ø–µ—Ä–µ—Å–ª—ñ–¥—É–≤–∞–≤ —Ü—é —Å—Ç—Ä–∞—Ç–µ–≥—ñ—á–Ω—É –º–µ—Ç—É' and \
        #    sent2 == '–ë—ñ–ª—å—à–∞ —á–∞—Å—Ç–∏–Ω–∞ –¥–µ—Ä–∂–∞–≤–Ω–∏—Ö —Ä–µ—Å—É—Ä—Å—ñ–≤ —Å—Ç–∞–ª–∞ –æ—Å–æ–±–∏—Å—Ç–æ—é –≤–ª–∞—Å–Ω—ñ—Å—Ç—é –ø—Ä–∞–≤–∏—Ç–µ–ª—è-–∞–≤—Ç–æ–∫—Ä–∞—Ç–∞ —ñ –π–æ–≥–æ –Ω–∞–π–±–ª–∏–∂—á–æ–≥–æ –æ—Ç–æ—á–µ–Ω–Ω—è':
        #     print("#")
        synonyms = pair["synonyms"]
        sent1_target_pos = False
        sent2_target_pos = False
        

        for synonym in synonyms:
            if not sent1_target_pos:
                sent1_target_pos = lemmatizer.get_target_idx(sent1, synonym["lemma"])
            if not sent2_target_pos:
                sent2_target_pos = lemmatizer.get_target_idx(sent2, synonym["lemma"])

        # if sent1_target_pos == False:
        #     print(f'NO POSITIONS FOUND FOR SENT_1 | {sent1} | {lemma}')
            
        # if sent2_target_pos == False:
        #     print(f'NO POSITIONS FOUND FOR SENT_2 | {sent2} | {lemma}')
            
        if sent1_target_pos == False or sent2_target_pos == False:
            bad_data.append(pair)
            continue
        
        # If both positions are found, add to good_data
        pair["sent1_target_pos"] = sent1_target_pos
        pair["sent2_target_pos"] = sent2_target_pos
        good_data.append(pair)

    return good_data, bad_data
        


def main():
    lemmatizer = Lemmatizer(input_file_path)

    # Lemmatize a word
    # res = lemmatizer.get_target_idx(".,–ª—ñ–∫–∞—Ä–∫–∞-—Ç–æ–º—É? –ø–æ—Ç—è–≥–∞–º–∏,.", "–ø–æ—Ç—è–≥")
    # res = lemmatizer.get_target_idx("–•–ª–æ–ø—Ü—ñ –≥—Ä–∞–ª–∏ —É —Ñ—É—Ç–±–æ–ª–∞ . –í–æ—Ä–æ—Ç–∞—Ä–µ–º –±—É–≤ –í–∞—Å–∏–ª—å... –ú'—è—á –ª–µ—Ç—ñ–≤ –ø—Ä–æ—Å—Ç–æ –Ω–∞ –≤–æ—Ä–æ—Ç–∞",
    #                                 "–≤–æ—Ä–æ—Ç–∞")
    
    # res = lemmatizer.get_target_idx("–û–¥ –¥–æ—Ä–æ–≥–∏ –π–æ–≥–æ –≤—ñ–¥–æ–∫—Ä–µ–º–ª—é–≤–∞–ª–∞ –Ω–µ–≥–ª–∏–±–æ–∫–∞ —Å—Ç–∞—Ä–∞ –∫–∞–Ω–∞–≤–∞, –ø–æ—Ä–æ—Å–ª–∞ –ª—ñ—â–∏–Ω–æ—é —ñ –ª–æ–∑–∞–º–∏", 
                                    # "–≤—ñ–¥–æ–∫—Ä–µ–º–ª—é–≤–∞—Ç–∏")
    # print(res)

    # =============================================================================

    with open(test_forms_file_path, 'r', encoding='utf-8') as file:
        pairs_test_df = [json.loads(line) for line in file]

    test_good, test_bad = df_lemmantization(pairs_test_df, lemmatizer)

    # Write the test set to a new file
    with open(test_good_file_path, 'w', encoding='utf-8') as output_file:
        for entry in test_good:
            json.dump(entry, output_file, ensure_ascii=False)
            output_file.write('\n')

    # Write the test set to a new file
    with open(test_bad_file_path, 'w', encoding='utf-8') as output_file:
        for entry in test_bad:
            json.dump(entry, output_file, ensure_ascii=False)
            output_file.write('\n')

    #* Test: good [267_913] | bad [[16_125]]
    #* after changing: Test: good [264_518] | bad [[19_520]]
    print(f"Test: good [{len(test_good)}] | bad [[{len(test_bad)}]]")
    # =============================================================================

    with open(train_forms_file_path, 'r', encoding='utf-8') as file:
        pairs_train_df = [json.loads(line) for line in file]

    train_good, train_bad = df_lemmantization(pairs_train_df, lemmatizer)

    # Write the train set to a new file
    with open(train_good_file_path, 'w', encoding='utf-8') as output_file:
        for entry in train_good:
            json.dump(entry, output_file, ensure_ascii=False)
            output_file.write('\n')

    # Write the train set to a new file
    with open(train_bad_file_path, 'w', encoding='utf-8') as output_file:
        for entry in train_bad:
            json.dump(entry, output_file, ensure_ascii=False)
            output_file.write('\n')

    #* Train: good [802_065] | bad [[50_045]]
    #* after changing: Train: good [797_629] | bad [[54_481]]
    print(f"Train: good [{len(train_good)}] | bad [[{len(train_bad)}]]")
    # =============================================================================



    # with open(pairs_file_path, 'r', encoding='utf-8') as file:
    #     pairs_data = [json.loads(line) for line in file]

    # for line in pairs_data:
    #     lemma, pairs = next(iter(line.items()))
    #     # line_expanded = {
    #     #     "lemma": lemma,
    #     #     "pairs": pairs,
    #     #     "positive_pairs": [],
    #     #     "negative_pairs": [],
    #     #     }
 
    #     for i, pair in enumerate(pairs):
    #         sent1 = pair["sentence1"]
    #         sent2 = pair["sentence2"]
    #         lemma = pair["lemma"]

    #         # if lemma == '–∞–≤—Ç–æ–∫—Ä–∞—Ç' and \
    #         #    sent1 == '–ü–µ—Ä–µ–¥ —Å–≤–æ—ó–º –≤—Ç–æ—Ä–≥–Ω–µ–Ω–Ω—è–º –¥–æ –ì—Ä–µ—Ü—ñ—ó —Å—Ö—ñ–¥–Ω–∏–π –∞–≤—Ç–æ–∫—Ä–∞—Ç –Ω–∞—Å–ª—É—Ö–∞–≤—Å—è –ø–æ—Ä–∞–¥, —è–∫ –ø–æ–∑–±–∞–≤–∏—Ç–∏ –µ–ª–ª—ñ–Ω—ñ–≤ –ø—Ä–æ–¥–æ–≤–æ–ª—å—á–æ—ó –±–∞–∑–∏, —ñ —Ç–≤–µ—Ä–µ–∑–æ –ø–µ—Ä–µ—Å–ª—ñ–¥—É–≤–∞–≤ —Ü—é —Å—Ç—Ä–∞—Ç–µ–≥—ñ—á–Ω—É –º–µ—Ç—É' and \
    #         #    sent2 == '–ë—ñ–ª—å—à–∞ —á–∞—Å—Ç–∏–Ω–∞ –¥–µ—Ä–∂–∞–≤–Ω–∏—Ö —Ä–µ—Å—É—Ä—Å—ñ–≤ —Å—Ç–∞–ª–∞ –æ—Å–æ–±–∏—Å—Ç–æ—é –≤–ª–∞—Å–Ω—ñ—Å—Ç—é –ø—Ä–∞–≤–∏—Ç–µ–ª—è-–∞–≤—Ç–æ–∫—Ä–∞—Ç–∞ —ñ –π–æ–≥–æ –Ω–∞–π–±–ª–∏–∂—á–æ–≥–æ –æ—Ç–æ—á–µ–Ω–Ω—è':
    #         #     print("#")
    #         synonyms = pair["synonyms"]
    #         sent1_target_pos = False
    #         sent2_target_pos = False
    #         for synonym in synonyms:
    #             if not sent1_target_pos:
    #                 sent1_target_pos = lemmatizer.get_target_idx(sent1, synonym["lemma"])
    #             if not sent2_target_pos:
    #                 sent2_target_pos = lemmatizer.get_target_idx(sent2, synonym["lemma"])

    #         if sent1_target_pos == False:
    #             print(f'NO POSITIONS FOUND FOR SENT_1 | {sent1} | {lemma}')
    #         if sent2_target_pos == False:
    #             print(f'NO POSITIONS FOUND FOR SENT_2 | {sent2} | {lemma}')
            

if __name__ == '__main__':
    main()

##
# Tddest: good [70557] | bad [[4981]]
# Train: good [638112] | bad [[41720]]
# Total: 708,669
