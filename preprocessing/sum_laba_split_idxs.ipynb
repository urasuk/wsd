{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yurayano/PycharmProjects/wsd/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import stats as stats_data\n",
    "import json\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 373kB [00:00, 153MB/s]                     \n",
      "2024-03-06 21:54:29 INFO: Downloaded file to /Users/yurayano/stanza_resources/resources.json\n",
      "2024-03-06 21:54:29 INFO: Downloading default packages for language: uk (Ukrainian) ...\n",
      "2024-03-06 21:54:30 INFO: File exists: /Users/yurayano/stanza_resources/uk/default.zip\n",
      "2024-03-06 21:54:31 INFO: Finished downloading models and saved to /Users/yurayano/stanza_resources\n"
     ]
    }
   ],
   "source": [
    "stanza.download('uk') # download Ukrainian model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 21:54:31 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 373kB [00:00, 11.6MB/s]                    \n",
      "2024-03-06 21:54:31 INFO: Downloaded file to /Users/yurayano/stanza_resources/resources.json\n",
      "2024-03-06 21:54:32 INFO: Loading these models for language: uk (Ukrainian):\n",
      "===========================\n",
      "| Processor | Package     |\n",
      "---------------------------\n",
      "| tokenize  | iu          |\n",
      "| mwt       | iu          |\n",
      "| pos       | iu_charlm   |\n",
      "| lemma     | iu_nocharlm |\n",
      "===========================\n",
      "\n",
      "2024-03-06 21:54:32 INFO: Using device: cpu\n",
      "2024-03-06 21:54:32 INFO: Loading: tokenize\n",
      "2024-03-06 21:54:32 INFO: Loading: mwt\n",
      "2024-03-06 21:54:32 INFO: Loading: pos\n",
      "2024-03-06 21:54:32 INFO: Loading: lemma\n",
      "2024-03-06 21:54:32 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza_nlp = stanza.Pipeline('uk', processors='tokenize,mwt,pos,lemma') # initialize Ukrainian neural pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  [\n",
       "    {\n",
       "      \"id\": 1,\n",
       "      \"text\": \"Ð±ÑƒÐ´ÑŒ-ÐºÐ¾Ð»Ð¸\",\n",
       "      \"lemma\": \"Ð±ÑƒÐ´ÑŒ-ÐºÐ¾Ð»Ð¸\",\n",
       "      \"upos\": \"ADV\",\n",
       "      \"xpos\": \"Pi------r\",\n",
       "      \"feats\": \"PronType=Int\",\n",
       "      \"start_char\": 0,\n",
       "      \"end_char\": 9\n",
       "    },\n",
       "    {\n",
       "      \"id\": 2,\n",
       "      \"text\": \"ÐÐµÑ‡ÑƒÐ¹\",\n",
       "      \"lemma\": \"ÐÐµÑ‡ÑƒÐ¹\",\n",
       "      \"upos\": \"PROPN\",\n",
       "      \"xpos\": \"Npmsny\",\n",
       "      \"feats\": \"Animacy=Anim|Case=Nom|Gender=Masc|NameType=Giv|Number=Sing\",\n",
       "      \"start_char\": 10,\n",
       "      \"end_char\": 15,\n",
       "      \"misc\": \"SpaceAfter=No\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 3,\n",
       "      \"text\": \"-\",\n",
       "      \"lemma\": \"-\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \"U\",\n",
       "      \"feats\": \"PunctType=Hyph\",\n",
       "      \"start_char\": 15,\n",
       "      \"end_char\": 16,\n",
       "      \"misc\": \"SpaceAfter=No\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 4,\n",
       "      \"text\": \"Ð›ÐµÐ²Ð¸Ñ†ÑŒÐºÐ¸Ð¹\",\n",
       "      \"lemma\": \"Ð›ÐµÐ²Ð¸Ñ†ÑŒÐºÐ¸Ð¹\",\n",
       "      \"upos\": \"PROPN\",\n",
       "      \"xpos\": \"Npmsny\",\n",
       "      \"feats\": \"Animacy=Anim|Case=Nom|Gender=Masc|NameType=Sur|Number=Sing\",\n",
       "      \"start_char\": 16,\n",
       "      \"end_char\": 25,\n",
       "      \"misc\": \"SpaceAfter=No\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 5,\n",
       "      \"text\": \",\",\n",
       "      \"lemma\": \",\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \"U\",\n",
       "      \"start_char\": 25,\n",
       "      \"end_char\": 26\n",
       "    },\n",
       "    {\n",
       "      \"id\": 6,\n",
       "      \"text\": \"Ñ‚Ð°Ñ€Ð³ÐµÑ‚Ð¾Ð¼\",\n",
       "      \"lemma\": \"Ñ‚Ð°Ñ€Ð³ÐµÑ‚\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"Ncmsiy\",\n",
       "      \"feats\": \"Animacy=Anim|Case=Ins|Gender=Masc|Number=Sing\",\n",
       "      \"start_char\": 27,\n",
       "      \"end_char\": 35,\n",
       "      \"misc\": \"SpaceAfter=No\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 7,\n",
       "      \"text\": \"!\",\n",
       "      \"lemma\": \"!\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \"U\",\n",
       "      \"start_char\": 35,\n",
       "      \"end_char\": 36\n",
       "    },\n",
       "    {\n",
       "      \"id\": 8,\n",
       "      \"text\": \"Ñ‡Ð¸\",\n",
       "      \"lemma\": \"Ñ‡Ð¸\",\n",
       "      \"upos\": \"CCONJ\",\n",
       "      \"xpos\": \"Ccs\",\n",
       "      \"start_char\": 37,\n",
       "      \"end_char\": 39\n",
       "    },\n",
       "    {\n",
       "      \"id\": 9,\n",
       "      \"text\": \"Ð½Ñ–\",\n",
       "      \"lemma\": \"Ð½Ñ–\",\n",
       "      \"upos\": \"INTJ\",\n",
       "      \"xpos\": \"I\",\n",
       "      \"start_char\": 40,\n",
       "      \"end_char\": 42,\n",
       "      \"misc\": \"SpaceAfter=No\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 10,\n",
       "      \"text\": \"...;\",\n",
       "      \"lemma\": \"...;\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \"U\",\n",
       "      \"start_char\": 42,\n",
       "      \"end_char\": 46,\n",
       "      \"misc\": \"SpaceAfter=No\"\n",
       "    }\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = stanza_nlp(\"Ð±ÑƒÐ´ÑŒ-ÐºÐ¾Ð»Ð¸ ÐÐµÑ‡ÑƒÐ¹-Ð›ÐµÐ²Ð¸Ñ†ÑŒÐºÐ¸Ð¹, Ñ‚Ð°Ñ€Ð³ÐµÑ‚Ð¾Ð¼! Ñ‡Ð¸ Ð½Ñ–...;\")\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "ÑÐ²Ñ–Ñ‚\n",
      "0\n",
      "4\n",
      "===\n",
      "2\n",
      "Ð±ÑƒÐ´ÑŒ-Ð»Ð°ÑÐºÐ°\n",
      "5\n",
      "15\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "for token in document.iter_words():\n",
    "    print(token.id)\n",
    "    print(token.lemma)\n",
    "    print(token.start_char)\n",
    "    print(token.end_char)\n",
    "    print(\"===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = stats_data.train_file_path\n",
    "test_file_path = stats_data.test_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../' + train_file_path, 'r', encoding='utf-8') as file:\n",
    "    train_pairs = [json.loads(line) for line in file]\n",
    "with open('../' + test_file_path, 'r', encoding='utf-8') as file:\n",
    "    test_pairs = [json.loads(line) for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatizer:\n",
    "    def __init__(self):\n",
    "        # Initialize your lemmatizer, assuming 'uk' is the language code for Ukrainian\n",
    "        self.lemmantizer = stanza.Pipeline('uk', processors='tokenize,mwt,pos,lemma')\n",
    "        \n",
    "    def _get_word_idx(self, lemantized_sent, word):\n",
    "        lemantized_sent_lemmas = [word_sent[\"lemma\"] for word_sent in lemantized_sent]\n",
    "        try:\n",
    "            index = lemantized_sent_lemmas.index(word[\"lemma\"])\n",
    "\n",
    "            word_idx = lemantized_sent[index].copy()\n",
    "            word_idx[\"start_word\"] = index      # inclusive\n",
    "            word_idx[\"end_word\"] = index + 1    # exclusive\n",
    "            return word_idx\n",
    "        except ValueError:\n",
    "            return -1\n",
    "\n",
    "    def _stem_sentence(self, input_entity, is_arr=False):\n",
    "        if is_arr:\n",
    "            sent = ' '.join(input_entity)\n",
    "        else:\n",
    "            sent = input_entity            \n",
    "            \n",
    "        doc = self.lemmantizer(sent)\n",
    "\n",
    "        sentence_lemantized = []\n",
    "        for word in doc.iter_words():\n",
    "            sentence_lemantized.append({\n",
    "                \"text\": word.text,\n",
    "                \"lemma\": word.lemma,\n",
    "                \"start_char\": word.start_char,\n",
    "                \"end_char\": word.end_char,\n",
    "            })\n",
    "\n",
    "        return sentence_lemantized\n",
    "\n",
    "    def get_target_idx(self, sentence: str, word: str, synonyms):\n",
    "        synonyms_lemma = [synonym[\"lemma\"] for synonym in synonyms]\n",
    "        lemantized_synonyms = self._stem_sentence(synonyms_lemma, True)\n",
    "        lemantized_sentence = self._stem_sentence(sentence)\n",
    "        \n",
    "\n",
    "        target_idxs = []\n",
    "        for syn_lemma in lemantized_synonyms:\n",
    "            word_idx = self._get_word_idx(lemantized_sentence, syn_lemma) \n",
    "            if word_idx != -1:\n",
    "                target_idxs.append(word_idx)\n",
    "\n",
    "        # filtered_target_idxs = [value for value in target_idxs if value != -1]\n",
    "        if len(target_idxs) != 1:\n",
    "            raise ValueError(\"ðŸ“Œ No target indexes found!\")\n",
    "\n",
    "        return target_idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = Lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Ð¡Ð¾Ð½Ñ†Ðµ Ð²Ð¶Ðµ Ð·Ð²ÐµÑ€Ð½ÑƒÐ»Ð¾ Ð· Ð¾Ð±Ñ–Ð´Ñƒ Ñ– Ñ…Ð¸Ð»Ð¸Ñ‚ÑŒÑÑ Ð½Ð° Ð·Ð°Ñ…Ñ–Ð´',\n",
       " 'sentence2': 'Ð—Ð°Ð±ÑƒÐ²Ð°Ð»Ð°   Ð¿Ñ€Ð¾ Ð”Ð¾Ñ€ÐºÑƒ Ñ– Ð²ÑÑ– Ð´ÑƒÐ¼ÐºÐ¸ Ð·Ð²ÐµÑ€Ð½ÑƒÐ»Ð° Ð½Ð° ÑÐµÐ±Ðµ',\n",
       " 'label': 0,\n",
       " 'lemma': 'Ð·Ð²ÐµÑ€Ð½ÑƒÑ‚Ð¸',\n",
       " 'synonyms': [{'lemma': 'Ð·Ð²ÐµÑ€Ð½ÑƒÑ‚Ð¸', 'accent_positions': [5]},\n",
       "  {'lemma': 'Ð·Ð²ÐµÑ€Ñ‚Ð°Ñ‚Ð¸', 'accent_positions': [5]}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_entry = train_pairs[0]\n",
    "train_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Ð·Ð²ÐµÑ€Ð½ÑƒÐ»Ð¾',\n",
       "  'lemma': 'Ð·Ð²ÐµÑ€Ð½ÑƒÑ‚Ð¸',\n",
       "  'start_char': 10,\n",
       "  'end_char': 18,\n",
       "  'start_word': 2,\n",
       "  'end_word': 3}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_idx_sent1 = lemmatizer.get_target_idx(train_entry[\"sentence1\"], train_entry[\"lemma\"], train_entry[\"synonyms\"])\n",
    "target_idx_sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Ð·Ð²ÐµÑ€Ð½ÑƒÐ»Ð°',\n",
       "  'lemma': 'Ð·Ð²ÐµÑ€Ð½ÑƒÑ‚Ð¸',\n",
       "  'start_char': 33,\n",
       "  'end_char': 41,\n",
       "  'start_word': 6,\n",
       "  'end_word': 7}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_idx_sent1 = lemmatizer.get_target_idx(train_entry[\"sentence2\"], train_entry[\"lemma\"], train_entry[\"synonyms\"])\n",
    "target_idx_sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ðŸ“Œ No target indexes found!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_pair \u001b[38;5;129;01min\u001b[39;00m train_pairs:\n\u001b[0;32m----> 2\u001b[0m     sent1_target_idx \u001b[38;5;241m=\u001b[39m \u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_target_idx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pair\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_pair\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlemma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_pair\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msynonyms\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     sent2_target_idx \u001b[38;5;241m=\u001b[39m lemmatizer\u001b[38;5;241m.\u001b[39mget_target_idx(train_pair[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence2\u001b[39m\u001b[38;5;124m\"\u001b[39m], train_pair[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemma\u001b[39m\u001b[38;5;124m\"\u001b[39m], train_pair[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msynonyms\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m     train_pair[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msent1_target_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sent1_target_idx\n",
      "Cell \u001b[0;32mIn[10], line 51\u001b[0m, in \u001b[0;36mLemmatizer.get_target_idx\u001b[0;34m(self, sentence, word, synonyms)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# filtered_target_idxs = [value for value in target_idxs if value != -1]\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target_idxs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ“Œ No target indexes found!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m target_idxs\n",
      "\u001b[0;31mValueError\u001b[0m: ðŸ“Œ No target indexes found!"
     ]
    }
   ],
   "source": [
    "for train_pair in train_pairs:\n",
    "    sent1_target_idx = lemmatizer.get_target_idx(train_pair[\"sentence1\"], train_pair[\"lemma\"], train_pair[\"synonyms\"])\n",
    "    sent2_target_idx = lemmatizer.get_target_idx(train_pair[\"sentence2\"], train_pair[\"lemma\"], train_pair[\"synonyms\"])\n",
    "    train_pair[\"sent1_target_idx\"] = sent1_target_idx\n",
    "    train_pair[\"sent2_target_idx\"] = sent2_target_idx\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
